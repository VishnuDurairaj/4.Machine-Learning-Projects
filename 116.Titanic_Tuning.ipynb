{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(r\"C:\\Users\\Vishnu\\Desktop\\datas\\titanic\\train.csv\")\n",
    "test_data=pd.read_csv(r\"C:\\Users\\Vishnu\\Desktop\\datas\\titanic\\test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking for imbalanced datd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    549\n",
       "1    342\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.Survived.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Yes it is imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Age column has 177 missing values and cabin column has 687 missing values and Embarked column has 2 missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Removing Unwanted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_features = ['PassengerId','Ticket','Cabin']\n",
    "\n",
    "train_data.drop(useless_features,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived      0\n",
       "Pclass        0\n",
       "Name          0\n",
       "Sex           0\n",
       "Age         177\n",
       "SibSp         0\n",
       "Parch         0\n",
       "Fare          0\n",
       "Embarked      2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating function to do all the feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create new feature -Age_Group\n",
    "\n",
    "def age_group(age):\n",
    "  \n",
    "    if age<=20:\n",
    "        return 'Children'\n",
    "    elif age<=30:\n",
    "        return 'Adult'\n",
    "    elif age <=45:\n",
    "        return 'Middle Aged'\n",
    "    elif age<=60:\n",
    "        return 'Old'\n",
    "    else:\n",
    "        return 'Senier Citizen'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract Title\n",
    "\n",
    "def get_title(name):\n",
    "    \n",
    "    return re.findall(r'([A-Za-z]+)\\.',name)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to do all feature engineering on both test and train data\n",
    "\n",
    "def preprocessing(training_data,testing_data):\n",
    "    \n",
    "    # Handling Imbalanced Data - Upsampling\n",
    "    \n",
    "    minority_ind=training_data[training_data.Survived==1].index.to_list()\n",
    "    new_ind=np.random.choice(minority_ind,207)\n",
    "    up_sampling=train_data.iloc[new_ind]\n",
    "    training_data=pd.concat([train_data,up_sampling],axis=0)\n",
    "    \n",
    "# Step 1 > Droping unwanted features\n",
    "\n",
    "    useless_features = ['PassengerId','Ticket','Cabin']\n",
    "    training_data.drop(useless_features,axis=1,inplace=True)\n",
    "    testing_data.drop(useless_features,axis=1,inplace=True)\n",
    "    \n",
    "# Step 2 > Handling missing value\n",
    "\n",
    "    training_data.Age.fillna(training_data.Age.median(),inplace=True) # Filling the NaN value\n",
    "    \n",
    "    testing_data.Age.fillna(testing_data.Age.median(),inplace=True) # Filling the NaN value\n",
    "\n",
    "\n",
    "    training_data.Embarked.fillna(training_data.Embarked.mode(),inplace=True) # Filling the NaN value\n",
    "    \n",
    "    testing_data.Embarked.fillna(testing_data.Embarked.mode(),inplace=True) # Filling the NaN value\n",
    "    \n",
    "    \n",
    "# Step 3 > Handing Categorical features\n",
    "\n",
    "\n",
    "# One Hot Encoding - Sex\n",
    "    \n",
    "    sex=pd.get_dummies(training_data.Sex,drop_first=True)\n",
    "    training_data.Sex=sex\n",
    "    \n",
    "    sex=pd.get_dummies(testing_data.Sex,drop_first=True)\n",
    "    testing_data.Sex=sex\n",
    "\n",
    "# One Hot Encoding - Embarked\n",
    "\n",
    "    embarked_one_hot=pd.get_dummies(training_data.Embarked,drop_first=True)\n",
    "    training_data=pd.concat([training_data,embarked_one_hot],axis=1)\n",
    "    training_data.drop('Embarked',axis=1,inplace=True)\n",
    "    \n",
    "    embarked_one_hot=pd.get_dummies(testing_data.Embarked,drop_first=True)\n",
    "    testing_data=pd.concat([testing_data,embarked_one_hot],axis=1)\n",
    "    testing_data.drop('Embarked',axis=1,inplace=True)\n",
    "    \n",
    "    \n",
    "# Step 3 >>>> Creating New Features -Age Group\n",
    "\n",
    "    age_groupp=[training_data['Age'].apply(age_group)]\n",
    "    training_data['Age_Group']=age_groupp[0]\n",
    "    \n",
    "    age_groupp=[testing_data['Age'].apply(age_group)]\n",
    "    testing_data['Age_Group']=age_groupp[0]\n",
    "\n",
    "# Creating New Features -family_size\n",
    "\n",
    "    train_data[\"family_Size\"]=train_data[\"SibSp\"]+train_data[\"Parch\"]+1\n",
    "    \n",
    "    \n",
    "# One hot Encoding -Age_Gropu\n",
    "\n",
    "    Age_Gropu_one_hot=pd.get_dummies(training_data.Age_Group,drop_first=True)\n",
    "    training_data=pd.concat([training_data,Age_Gropu_one_hot],axis=1)\n",
    "    training_data.drop('Age_Group',axis=1,inplace=True)\n",
    "    \n",
    "    Age_Gropu_one_hot=pd.get_dummies(testing_data.Age_Group,drop_first=True)\n",
    "    testing_data=pd.concat([testing_data,Age_Gropu_one_hot],axis=1)\n",
    "    testing_data.drop('Age_Group',axis=1,inplace=True)\n",
    "    \n",
    "# Creating New Features -Title\n",
    "\n",
    "    training_data[\"Title\"]=[training_data.Name.apply(get_title)][0]\n",
    "    \n",
    "    training_data['Title'] = training_data['Title'].replace(['Mlle','Lady'], 'Miss')\n",
    "    training_data['Title'] = training_data['Title'].replace('Ms', 'Miss')\n",
    "    training_data['Title'] = training_data['Title'].replace(['Mme','Sir'], 'Mrs')\n",
    "    training_data['Title'] = training_data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don',\n",
    "                                                   'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "    training_data.drop('Name',axis=1,inplace=True)\n",
    "    \n",
    "    \n",
    "    testing_data[\"Title\"]=[testing_data.Name.apply(get_title)][0]\n",
    "    \n",
    "    testing_data['Title'] = testing_data['Title'].replace(['Mlle','Lady'], 'Miss')\n",
    "    testing_data['Title'] = testing_data['Title'].replace('Ms', 'Miss')\n",
    "    testing_data['Title'] = testing_data['Title'].replace(['Mme','Sir'], 'Mrs')\n",
    "    testing_data['Title'] = testing_data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don',\n",
    "                                                   'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "    testing_data.drop('Name',axis=1,inplace=True)\n",
    "    \n",
    "    \n",
    "# One hot Encoding -Title\n",
    "\n",
    "    Title_one_hot=pd.get_dummies(training_data.Title,drop_first=True)\n",
    "    training_data=pd.concat([training_data,Title_one_hot],axis=1)\n",
    "    training_data.drop('Title',axis=1,inplace=True)\n",
    "    \n",
    "    Title_one_hot=pd.get_dummies(testing_data.Title,drop_first=True)\n",
    "    testing_data=pd.concat([testing_data,Title_one_hot],axis=1)\n",
    "    testing_data.drop('Title',axis=1,inplace=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    return training_data,testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading training and testing file\n",
    "train_data=pd.read_csv(r\"C:\\Users\\Vishnu\\Desktop\\datas\\titanic\\train.csv\")\n",
    "test_data=pd.read_csv(r\"C:\\Users\\Vishnu\\Desktop\\datas\\titanic\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting clean data \n",
    "X,y=preprocessing(train_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    549\n",
       "0    549\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 1298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.Survived.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# def reduce_dimenstion(X,no_com):\n",
    "#     pca_model=PCA(n_components=no_com)\n",
    "#     Components=pca_model.fit_transform(X)\n",
    "#     return Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_X=reduce_dimenstion(X.iloc[:,1:],10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier,RandomForestClassifier\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the data\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X.iloc[:,1:],X.iloc[:,0].values,test_size=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_models(x_train,y_train,x_test,y_test):\n",
    "    \n",
    "    All_model=[]\n",
    "    result={}\n",
    "    prediction=[]\n",
    "    \n",
    "    Decision_Tree = DecisionTreeClassifier(max_depth=13)\n",
    "    Decision_Tree.fit(x_train,y_train)\n",
    "    predicted=Decision_Tree.predict(x_test)\n",
    "    result['Decision_Tree']=sum(predicted==y_test)/len(y_test)\n",
    "    prediction.append(predicted)\n",
    "    \n",
    "    GB_model = GradientBoostingClassifier(n_estimators=200,max_depth=13)\n",
    "    GB_model.fit(x_train,y_train)\n",
    "    predicted=GB_model.predict(x_test)\n",
    "    result['GB_model']=sum(predicted==y_test)/len(y_test)\n",
    "    prediction.append(predicted)\n",
    "    \n",
    "    XGB_model = xgboost.XGBClassifier(n_estimators=300,use_label_encoder=False,gamma=0.9,n_jobs=15,\n",
    "                                     learning_rate=0.5, max_delta_step=0, max_depth=15)\n",
    "    XGB_model.fit(x_train,y_train)\n",
    "    predicted=XGB_model.predict(x_test)\n",
    "    result['XGB_model']=sum(predicted==y_test)/len(y_test)\n",
    "    prediction.append(predicted)\n",
    "    \n",
    "    RF_model = RandomForestClassifier(n_estimators=300)\n",
    "    RF_model.fit(x_train,y_train)\n",
    "    predicted=RF_model.predict(x_test)\n",
    "    result['RF_model']=sum(predicted==y_test)/len(y_test)\n",
    "    prediction.append(predicted)\n",
    "    \n",
    "    new=np.swapaxes(prediction,0,1)\n",
    "    new_prediction=[]\n",
    "    \n",
    "    All_model.extend([Decision_Tree,GB_model,XGB_model,RF_model])\n",
    "    for i in new:\n",
    "        result1=Counter(i).most_common(1)\n",
    "        new_prediction.append(result1[0][0])\n",
    "    result['final_accuracy']=(sum(np.array(new_prediction)==y_test)/len(y_test))\n",
    "    return result,All_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:12:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "result,models_=best_models(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Decision_Tree': 0.9636363636363636,\n",
       " 'GB_model': 0.9636363636363636,\n",
       " 'XGB_model': 0.8909090909090909,\n",
       " 'RF_model': 0.9636363636363636,\n",
       " 'final_accuracy': 0.9818181818181818}"
      ]
     },
     "execution_count": 1277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the trained model using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the model in pickle\n",
    "\n",
    "import pickle\n",
    "\n",
    "titanic_trained_best = open(r'C:\\Users\\Vishnu\\Desktop\\jupyter Projects\\Pickle_Data\\titanic_trained_best.pickle',mode='wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1279,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(models_,titanic_trained_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1280,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_trained.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1281,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model=open(r'C:\\Users\\Vishnu\\Desktop\\jupyter Projects\\Pickle_Data\\titanic_trained_best.pickle','rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1282,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model=pickle.load(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making prediction using multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_function(X_test,models):\n",
    "    predictions=[]\n",
    "    for i in models:\n",
    "        predicted=i.predict(X_test)\n",
    "        predictions.append(predicted)\n",
    "    new_predicted=np.swapaxes(predictions,0,1)\n",
    "    \n",
    "    final_result=[]\n",
    "    for i in new_predicted:\n",
    "        _result=Counter(i).most_common(1)\n",
    "        final_result.append(_result[0][0])\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a missing value in Fare column in test data set \n",
    "y.Fare=y.Fare.fillna(y.Fare.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing dimension of test data\n",
    "# y=reduce_dimenstion(y.values,13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1293,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_predition=predict_function(y,models_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the predictions in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1294,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv(r\"C:\\Users\\Vishnu\\Desktop\\datas\\titanic\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1295,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.DataFrame({\"PassengerId\":test_data.PassengerId,\"Survived\":titanic_predition})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1296,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(r\"C:\\Users\\Vishnu\\Desktop\\jupyter Projects\\Titanic_prediction\\submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
